<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Research | Ourren]]></title>
  <link href="http://blog.ourren.com/categories/research/atom.xml" rel="self"/>
  <link href="http://blog.ourren.com/"/>
  <updated>2015-04-14T12:13:52-07:00</updated>
  <id>http://blog.ourren.com/</id>
  <author>
    <name><![CDATA[ourren]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[NLTK笔记:词干提取与词性标注]]></title>
    <link href="http://blog.ourren.com/2015/03/12/nltk_note_stem_pos_tag/"/>
    <updated>2015-03-12T00:00:00-07:00</updated>
    <id>http://blog.ourren.com/2015/03/12/nltk_note_stem_pos_tag</id>
    <content type="html"><![CDATA[<p>前面虽然对「nltk分句和分词」进行了分析和实现，但是在进一步进行处理之前，我们需要对这些词语进行规范化处理，例如我们需要统一所有单词的大小写，词语时态问题「相同的单词在不同的时态句型中属于不同的单词，我们需要转换为同一单词」等。nltk则针对问题提供了相关的类库，主要有两类操作：词干提取，词性标注。</p>

<h3>词干提取</h3>

<p>词干提取主要是将不同时态的词语转变为单词一般时态，例如seeing->see，extremely->extem，目前提供的类库在一定程度上可以实现这类转换「有些词语暂时不行，主要有些模块是基于字典的」。</p>

<!--more-->


<p>nltk提供三种方法可以实现如上功能：Porter、Lancaster、WordNet。使用都相当简单，具体使用方法如下：</p>

<pre><code>#! /usr/bin/env python
/# encoding: utf-8

"""
Copyright (c) 2014-2015 ourren
author: ourren &lt;i@ourren.com&gt;
"""
import nltk

def main():
    print 'enter main...'
    sentence = "Listen, strange women lying in ponds \
    distributing swords. Supreme executive power derives \
    from a mandate from the masses, not from some farcical aquatic ceremony"
    sentences = nltk.sent_tokenize(sentence)
    sentences = [nltk.word_tokenize(sent) for sent in sentences]
    for sent in sentences:
        # Porter
        porter = nltk.PorterStemmer()
        words = [porter.stem(word) for word in sent]
        print words

        # Lancaster
        lancaster = nltk.LancasterStemmer()
        lwords = [lancaster.stem(t) for t in sent]
        print lwords

        # WordNet
        wnl = nltk.WordNetLemmatizer()
        wwrods = [wnl.lemmatize(t) for t in sent]
        print wwrods

if __name__ == "__main__":
    main()
</code></pre>

<h3>词性标注</h3>

<p><a href="http://baike.baidu.com/view/377635.htm">词性</a>指作为划分词类的根据的词的特点。现代汉语的词可以分为两类12种词性。一类是实词：名词、动词、形容词、数词、量词和代词。一类是虚词：副词、介词、连词、助词、拟声词和叹词。因此nltk对应的pos_tag模块也是实现这类功能，将一个句子中的词性进行标注；</p>

<p>nltk中将词汇按它们的词性(parts-of-speec h,POS)分类以及相应的标注它们的过程被称为词性标注(part-of-speech tagging, POS tagging)或干脆简称标注。其中标注结果中缩写词所代表的词性如下：</p>

<pre><code>ADJ     adjective   new, good, high, special, big, local
ADV     adverb  really, already, still, early, now
CNJ     conjunction and, or, but, if, while, although
DET     determiner  the, a, some, most, every, no
EX      existential there, there’s
FW      foreign word    dolce, ersatz, esprit, quo, maitre
MOD     modal verb  will, can, would, may, must, should
N       noun    year, home, costs, time, education
NP      proper noun Alison, Africa, April, Washington
NUM     number  twenty-four, fourth, 1991, 14:24
PRO     pronoun he, their, her, its, my, I, us
P       preposition on, of, at, with, by, into, under
TO      the word to to
UH      interjection    ah, bang, ha, whee, hmpf, oops
V       verb    is, has, get, do, make, see, run
VD      past tense  said, took, told, made, asked
VG      present participle  making, going, playing, working
VN      past participle given, taken, begun, sung
WH      wh determiner   who, which, when, what, where, how
</code></pre>

<p>下面看具体例如：</p>

<pre><code>def sentence_pos(sentences):
    for sent in sentences:
        words = nltk.pos_tag(sent)
        print words
</code></pre>

<p>完整代码可以见<a href="https://github.com/ourren/learn_nltk">learn_nltk</a>。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[NLTK笔记:分句与分词]]></title>
    <link href="http://blog.ourren.com/2015/02/21/nltk_note_token_sentence/"/>
    <updated>2015-02-21T00:00:00-08:00</updated>
    <id>http://blog.ourren.com/2015/02/21/nltk_note_token_sentence</id>
    <content type="html"><![CDATA[<p>NLTK在数据抓取完成后，你拿到的数据往往是一篇文章或者一大段文字，在进行其他处理之前，你需要先对文章进行切割或者处理(去除多余字符、特殊符号，分句和分词)，分句主要是可以把有些不需要的句子给去掉，比如长度小于10的。</p>

<h4>分句</h4>

<p>分句在nltk中没有提供相关的库来实现，但是我们可以通过python的split等函数快速完成切分任务，主要的分割特征如下：
+ 中文主要有(。？！)这几个句子结尾标志；
+ 英文也差不多(. ? !)；
使用split函数进行分割，可以得到新的列表，例如下面的函数;</p>

<!--more-->


<pre><code>def sentence_split(str_centence):
    list_ret = list()
    for s_str in str_centence.split('.'):
        if '?' in s_str:
            list_ret.extend(s_str.split('?'))
        elif '!' in s_str:
            list_ret.extend(s_str.split('!'))
        else:
            list_ret.append(s_str)
    return list_ret
</code></pre>

<h4>分词</h4>

<p>分词在NLP处理中运用得最多，但是目前针对英文分词基本上已经成熟，而针对中文的分词技术还在不断发展，对于pythoner而言，主要可以采取如下的分词方法对句子或者段落进行分词：</p>

<ul>
<li>中文：采用<a href="https://github.com/fxsjy/jieba">jieba</a>进行分词，然后再可以通过NLTK进行词频统计分析，整体而言jieba对中文分词还是可以接受；</li>
<li>英文：可以直接使用NLTK中nltk.tokenize模块进行分词；</li>
</ul>


<p>中文的分词实例可以参照这篇文章<a href="http://blog.ourren.com/?p=89252">中文分词与词频统计实例</a>,英文的分词示例如下：</p>

<pre><code>import nltk

def main():
    sentence = """At eight o'clock on Thursday morning Arthur didn't feel very good."""
    tokens = nltk.word_tokenize(sentence)
    print tokens

if __name__ == '__main__':
    main()
</code></pre>

<h4>特殊字符</h4>

<p>在处理分词或者分句中经常会除去一些特殊符号或者特殊单词，一般可能会用到的python函数：</p>

<ul>
<li>startswith</li>
<li>endswith</li>
<li>contains</li>
<li>replace</li>
<li>split</li>
<li>len</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[NLTK笔记:简介与环境搭建]]></title>
    <link href="http://blog.ourren.com/2015/02/05/nltk_note_environment_install/"/>
    <updated>2015-02-05T00:00:00-08:00</updated>
    <id>http://blog.ourren.com/2015/02/05/nltk_note_environment_install</id>
    <content type="html"><![CDATA[<h3>NLP与NLTK</h3>

<p>在详细介绍nltk之前，我感觉有必要区分下NLP与NLTK的关系:</p>

<p>NLP是指自然语言处理，英文(Natural Language Processing)，该技术主要解决计算机自动识别和处理人类语言，如何让计算机能够理解我们的语言，从而减少人类的工作量。个人认为其应用场景有如下几点：</p>

<ol>
<li>个性推荐，需要计算两类物品的相识度和相关度；</li>
<li>情感分析，用户对商品的评价，网民对帖子或者文章的态度；</li>
<li>关键字提取，对网络商品的标题提取并分类，自动摘要；</li>
<li>其它，待补充；</li>
</ol>


<p>并且该相关技术最近几年热度非常高，计算机行业内研究的人也越来越多，但是不同语言的处理差异很大，比如英文的各种语言处理库基本上已经成熟，而中文的语言处理则发展缓慢；</p>

<!--more-->


<p>而NLTK是斯坦福大学开发的处理自然语言的python库，(斯坦福大学自然语言处理组是世界知名的NLP研究小组，目前course上他们的课程暂时还没看开放)，因此我们可以借助NLTK库的一些功能来处理各种事情。事实上NLTK提供的功能相当全，主要包含如下功能：</p>

<ol>
<li>语料库：提供了经典书籍，词典，演讲，网络语言，论坛等各种语料库；</li>
<li>断句与分词：可以方便地对文章，段落进行分词；</li>
<li>词频统计：计算句子或者文章中每个单词的频率；</li>
<li>同义词与词态：单词的同义词「WordNet」和单词词态「过去式，进行时等」的还原；</li>
<li>词性的标注：动词，名词，形容词和副词等的标注；</li>
<li>分类算法：例如常见的信息熵，朴素贝叶斯算法，最大信息熵模型等；</li>
<li>情感分析：当然这个其实是利用语料库和分类算法的一种应用场景；</li>
<li>其它:待补充；</li>
</ol>


<p>说了这么多，NLTK其实主要是针对英语进行处理，对中文支持不行，其它博客也有<a href="http://www.52nlp.cn/python%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%AE%9E%E8%B7%B5-%E5%9C%A8nltk%E4%B8%AD%E4%BD%BF%E7%94%A8%E6%96%AF%E5%9D%A6%E7%A6%8F%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%99%A8">改造NLTK支持中文的博文</a>，NLP和NLTK的简介基本上就扯这么多。</p>

<h3>环境搭建</h3>

<p>NLTK的安装步骤非常简单，具体参照<a href="http://www.nltk.org/install.html">官网</a>：</p>

<pre><code>#Mac/Unix
Install Setuptools: http://pypi.python.org/pypi/setuptools
Install Pip: run sudo easy_install pip
Install Numpy (optional): run sudo pip install -U numpy
Install NLTK: run sudo pip install -U nltk
Test installation: run python then type import nltk
</code></pre>

<p>需要注意的是上面只安装了nltk的主库，语料库这些都没有安装，可以通过如下的命令安装语料库，当然这些语料库也可以在需要的时候再进行安装，在python命令行中运行如下命令：</p>

<pre><code>import nltk  
nltk.download()  
</code></pre>

<p>然后会弹出下载对话框，需要你选择语料库进行下载。在这里补充说明下什么是语料库：所谓语料库就是别人或者官方收集的测试数据，而这些数据一般已经进行了特殊处理，方便你在处理各种数据的时候可以参照这些语料库进行分析，当然你也可以加入自己的语料库。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[NLTK笔记:系列文章概述]]></title>
    <link href="http://blog.ourren.com/2015/02/02/nltk_note_content_list/"/>
    <updated>2015-02-02T00:00:00-08:00</updated>
    <id>http://blog.ourren.com/2015/02/02/nltk_note_content_list</id>
    <content type="html"><![CDATA[<p>从本系列文章中，我将最近学习自然语言处理中的NLTK库的相关技术进行分享，帮助初学者熟悉NLTK库的相关模块和功能，希望能够让你轻松地操作相关模块，完成自己的需求任务。</p>

<h3>文章目录</h3>

<p>系列文章首先会介绍NLTK的相关概念，然后介绍NLTK中比较常用的几个模块，争取每个篇文章都配上实例代码，最后用一个完整的实例进行讲解，目前暂定的目录如下：</p>

<!-- more -->


<ol>
<li>NLTK相关概念与环境搭建；</li>
<li>NLTK之分句；</li>
<li>NLTK之分词；</li>
<li>NLTK之句子分析；</li>
<li>NLTK之词性分析；</li>
<li>NLTK之词态分析；</li>
<li>NLTK之感情分析；</li>
<li>NLTK之关键字提取；</li>
<li>NLTK之分类算法；</li>
<li>&hellip;&hellip;</li>
</ol>


<p>暂时只考虑到以上的几个部分，以后有特别需要的部分再继续补上，其中本系列文章的技术主要参考两本书籍：</p>

<ol>
<li><a href="http://www.nltk.org/book/">Natural Language Processing With Python</a></li>
<li><a href="http://vdisk.weibo.com/s/qdxg3WEhv6A2">Machine Learning for Hackers</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[安全界的顶级会议]]></title>
    <link href="http://blog.ourren.com/2015/01/20/top_security_conference/"/>
    <updated>2015-01-20T00:00:00-08:00</updated>
    <id>http://blog.ourren.com/2015/01/20/top_security_conference</id>
    <content type="html"><![CDATA[<p>可能很多人(特别是国内的，咳咳)跟我一样读了很多年书，虽然从事的是安全领域的研究，但是国际上顶级的安全会议可能都不知道，我也是最近才研究了一下国际上的安全顶级会议和论文下载方法。</p>

<h4>四大顶会</h4>

<p>安全界有四大著名顶级会议，简称：S&amp;P、CCS、Security、NDSS；其实有两个网页对安全类会议进行了排名，详细排名大家可以参考「1、2」。但是貌似从事安全研究的人员只认这四个会议，导致这四个会议论文的通过率非常低，因此我们只要关注这四个会议的文章就大概知道国际上安全人员在研究些啥东西了；</p>

<!--more-->


<ol>
<li><p>S&amp;P</p>

<p> 从<a href="http://www.ieee-security.org/TC/SP-Index.html">S&amp;P</a>的官方上看，你会发现其实S&amp;P每年不只一个会议，S&amp;P又分为两类：SP Conference Information、SP Workshops Information。第一类「SPC」大家一看英文应该就知道含义吧，第二类可以看2014年<a href="http://www.ieee-security.org/TC/SPW2014/index.html">官网</a>的一个介绍：</p>

<blockquote><p>Overview:
Since 1980, the IEEE Symposium on Security and Privacy (SP) has been the premier forum for the presentation of developments in computer security and electronic privacy, and for bringing together researchers and practitioners in the field.</p>

<p>In order to further expand the opportunities for scientific exchanges, we created a new venue within the IEEE CS Technical Committee on Security and Privacy called Security and Privacy Workshops (SPW). The typical purpose of such a workshop is to cover a specific aspect of security and privacy in more detail, making it easy for the participants to attend IEEE SP and a specialized workshop at IEEE SPW with just one trip. Furthermore, the co-location offers synergies for the organizers. The workshops are co-located with the IEEE Security and Privacy Symposium, and the number of workshops and attendees have grown steadily during recent years. Workshops can be annual events (e.g. W2SP), one time events, or aperiodic.</p></blockquote>

<p>  说了这么多的意思就是「SPW」是为了增加投论文的机会，毕竟「SP」每年也收录不到太多的论文，并且这个「SPW」所囊括的会议也越来越多，就2014年貌似有就有<a href="http://www.ieee-security.org/TC/SPW2014/index.html">7个会议</a>；</p>

<p>  这个会议的演讲论文在<a href="http://www.ieee-security.org/TC/SP-Index.html">主页</a>以及每个会议的<a href="http://www.ieee-security.org/TC/SPW2014/index.html">首页</a>(需要点击进入每个会议网页哈)都有下载，另外你要是你拥有IEEE论文数据库，可以通过<a href="http://ieeexplore.ieee.org/xpl/conferences.jsp">IEEE的会议搜索</a>进行会议搜索，比如：<a href="http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=6954656">Security and Privacy (SP), 2014 IEEE Symposium on</a>、<a href="http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=6954698">Security and Privacy Workshops (SPW), 2014 IEEE</a>。</p></li>
<li><p>CCS</p>

<p> CCS的全称为：<a href="http://www.sigsac.org/ccs.html">ACM Conference on Computer and Communications Security</a>, 看介绍也是开始于1993，还是挺久远的一个会议。</p>

<p> 会议的论文可以通过网站的链接看到<a href="http://dl.acm.org/event.cfm?id=RE182&amp;tab=pubs&amp;CFID=619951631&amp;CFTOKEN=27784456">历年的论文记录</a>,当然这个论文库是在acm数据库中，你没有这个论文库的话果断Google吧，或者使用<a href="http://www.informatik.uni-trier.de/~ley/db/">dblp数据库</a>进行搜索，一般都可以搜到PDF，主要还是源于计算机和安全领域大家都还是乐于分享。2014年CCS的所有论文可以通过<a href="http://dl.acm.org/citation.cfm?id=2666652&amp;CFID=619951631&amp;CFTOKEN=27784456">Proceedings of the 2014 Workshop on Artificial Intelligent and Security Workshop</a>进行下载。</p></li>
<li><p>USENIX Security</p>

<p> 终于该介绍USENIX Security了，其实本人最开始接触的就是这个会议，因此这个会议的论文集居然有epub和mobi格式，不得不说科技很强大。USENIX最开始其实是UNIX，但是由于商标问题后来改为USENIX，具体过程可以参见<a href="http://zh.wikipedia.org/wiki/USENIX">维基百科USENIX</a>。USENIX 其实是一个计算机类会议的总称，详细会议列表可以看<a href="https://www.usenix.org/conferences">这里</a>，而USENIX Security只是USENIX中的安全会议，并且USENIX Security会议涵盖的安全领域也非常多，包含：二进制安全、固件安全、取证分析、Web安全、隐私保护、恶意分析等。</p>

<p> 会议的论文直接在<a href="https://www.usenix.org/conference/usenixsecurity14/technical-sessions">官网</a>提供下载，也有很多格式(PDF、EPUB、MOBI)。</p></li>
<li><p>NDSS</p>

<p> <a href="http://www.internetsociety.org/events/ndss-symposium">NDSS</a>的全称为(Network and Distributed System Security)，其官网中也提供了<a href="http://www.internetsociety.org/events/ndss-symposium/previous-conferences">历届会议列表</a>。
 NDSS2014年的论文集可以通过这里进行<a href="http://pan.baidu.com/wap/link?uk=1812237392&amp;shareid=3622732641&amp;third=0">下载</a>。</p></li>
</ol>


<h4>参考链接</h4>

<ol>
<li><a href="http://faculty.cs.tamu.edu/guofei/sec_conf_stat.htm">Computer Security Conference Ranking and Statistic</a></li>
<li><a href="http://icsd.i2r.a-star.edu.sg/staff/jianying/conference-ranking.html">Top Crypto and Security Conferences Ranking (2014)</a></li>
<li></li>
</ol>

]]></content>
  </entry>
  
</feed>
