<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Research | Ourren]]></title>
  <link href="http://blog.ourren.com/blog/categories/research/atom.xml" rel="self"/>
  <link href="http://blog.ourren.com/blog/"/>
  <updated>2015-09-19T12:40:15-07:00</updated>
  <id>http://blog.ourren.com/blog/</id>
  <author>
    <name><![CDATA[ourren]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[论文写作环境: TeXstudio+LanguageTool+Git+latexdiff]]></title>
    <link href="http://blog.ourren.com/blog/2015/09/19/xie-zuo-huan-jing/"/>
    <updated>2015-09-19T11:13:32-07:00</updated>
    <id>http://blog.ourren.com/blog/2015/09/19/xie-zuo-huan-jing</id>
    <content type="html"><![CDATA[<p>古时候有“孟母三迁”的故事告诉我们：环境影响人的成长；而当代社会由于科技的快速发展，办公环境和工具的选择会直接影响人们的办公效率和工作的热情。高富帅公司往往以优秀的工作环境「高薪，Mac全套，人工学电脑椅，空气净化器等」来吸引应聘者，屌丝公司则是以人文情怀拉拢一堆基友。论文写作环境也不例外，极客者善于利用各种工具打造一套完整的写作、投稿与修正的工具链。个人虽然接触写作时间不长，但自认为高级的论文写作环境需要满足如下几项：</p>

<ul>
<li>作者不用担心格式问题「字体大小，间距，编号」；</li>
<li>格式转换起来比较方便「方便投递不同期刊」；</li>
<li>方便作者之间进行修改「对比修改前后差异」；</li>
<li>语法句式检测「国人很常见的问题了」；</li>
</ul>


<p>鉴于如上的写作环境需求，个人在实践中总结了一套完整的写作工具链，现分享如下：</p>

<h4>书写工具</h4>

<p>初学者最常用的就是Word了，虽然各大会议或者期刊都提供了论文的Word模板，但是实际书写和排版过程中经常会遇到各种问题，解决起来非常麻烦。鉴于以上的各种经验，本人最终转向了LaTex。<a href="https://www.latex-project.org/">LaTeX</a>跟<a href="http://daringfireball.net/projects/markdown/">Markdown</a>的理念差不多，首先定义了一套书写规则，然后你按照其规则进行书写，最后你就可以生成一份完美的文档「譬如本文就是采用markdown进行书写的」，只不过LaTeX会相对复杂很多，主要针对学术作。支持LaTex的编辑器也很多，列举如下：</p>

<ul>
<li><a href="http://www.winedt.com/">WinEdt</a>:Windows下的写作利器，可惜是收费的。</li>
<li><a href="https://www.overleaf.com/">overleaf</a>: 在线LaTex写作平台，免费版有些限制，但是基本上够用了；</li>
<li><a href="http://www.texstudio.org/">TeXstudio</a>编辑器属于开源之作，免费并且跨平台，并且支持双栏同时显示「源文件,PDF预览」，可以说这个绝对是大屏显示器的福音。</li>
</ul>


<p><img src="http://ourren.b0.upaiyun.com/texstudio.png" alt="TeXstudio" /></p>

<h4>拼写与语法检测</h4>

<p>论文写作与投稿中遇到最多的问题就是语言问题，已有的语法检测工具如下：</p>

<ul>
<li>Word审阅中提供拼写和语法检测；</li>
<li><a href="http://www.hemingwayapp.com/desktop.html">Hemingway Editor 2</a>提供语法和句型优化功能；</li>
<li><a href="https://languagetool.org/">LanguageTool</a>: 开源的文体和语法校正的软件, 支持多种语言，并且可以集成到其他软件, 比如LanguageTool集成到TeXStudio<a href="http://wiki.languagetool.org/checking-la-tex-with-languagetool">Checking (La)TeX With LanguageTool</a>；</li>
<li>一些其它的小工具，比如<a href="https://github.com/invernizzi/Chrisper">Chrisper</a>;</li>
</ul>


<p><img src="http://ourren.b0.upaiyun.com/grammar.png" alt="TeXstudio" /></p>

<h4>版本管理</h4>

<p>一篇论文从初稿到发表的过程中会生成不同的论文版本，而<a href="https://git-scm.com/">Git</a>的分布式管理可以让我们轻松管理不同版本的论文，而<a href="https://gitlab.com/">gitlab</a>, <a href="https://git.oschina.net/">oschina</a>, <a href="http://coding.net/">coding</a>等则提供了私有项目的功能，可以方便帮助我们管理论文版本。</p>

<h4>修改与批注</h4>

<p><img src="http://ourren.b0.upaiyun.com/latexdiff.png" alt="TeXstudio" /></p>

<p>首先不得不说Word中的批注和删改功能做得非常不错，而<a href="https://www.ctan.org/tex-archive/support/latexdiff">latexdiff</a>插件的出现让LaTex也具有批注和删除功能，并且使用简单「参考说明文档即可」，效果显示也不错。</p>

<h4>写作攻击链</h4>

<p>通过如上的各种工具写作，最终选择的写作工具如下：</p>

<ul>
<li>写作: TeXstudio；</li>
<li>拼写与语法检测: LanguageTool；</li>
<li>版本控制: Gitlab；</li>
<li>修改批注: latexdiff.</li>
</ul>


<h4>其它资料</h4>

<ul>
<li><a href="http://blog.ourren.com/2015/04/17/latex-skils-summary/">LaTeX写作技巧总结</a>;</li>
<li><a href="https://github.com/secdr/research-method">论文写作与资料分享</a>;</li>
<li><a href="http://www.latextemplates.com/">LaTeX Templates</a>;</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Measuring and Detecting Malware Downloads in Live Network Traffic]]></title>
    <link href="http://blog.ourren.com/blog/2015/07/03/measuring-and-detecting-malware-downloads-in-live-network-traffic/"/>
    <updated>2015-07-03T00:32:58-07:00</updated>
    <id>http://blog.ourren.com/blog/2015/07/03/measuring-and-detecting-malware-downloads-in-live-network-traffic</id>
    <content type="html"><![CDATA[<p>今天分享的是Roberto Perdisci教授在2013年发表在ESORICS上的一篇论文，该文在Google学术中虽然引用不高，单独读完之后感觉在分类前提取特征的方法写得不错，分享给大家。</p>

<h3>概述</h3>

<p>论文主要观点：</p>

<p>通过在网络出口部署相关设备，可以观察局域网内所有软件下载行为，通过一系列的行为特征和恶意软件的标签即可实现恶意软件的分类，并且可以发现未知恶意软件，同时该分类效果比谷歌浏览器的恶意检测系统在一定程度上误报率要低。</p>

<p>论文主要成果：</p>

<ul>
<li>提出了一种在网络数据包中检测恶意软件下载行为的模型；</li>
<li>通过实际部署发现效果还不错，可以发现一些未知的恶意样本；</li>
<li>通过对比发现，模型的误报率其实挺低，准确率也挺高的。</li>
</ul>


<!--more-->


<h3>模型框图</h3>

<p><img src="http://blog.ourren.com/upload/amico.png" alt="amico" /></p>

<h3>创新技术点</h3>

<p>个人感觉论文主要的亮点在于对这种检测模型中的特征的挖掘和阐述，其次文章对模型和整个研究内容的细节和死角都做了很详细的分析和阐述，让你找不出破绽。另外作者的测试方法也还是蛮有意思的。</p>

<p>这里重点阐述作者模型中的特征选取问题：</p>

<h4>历史下载数据。</h4>

<p>通过分析连续一段时间文件的下载次数，多少客户下载，第一次下载是多少天以前，一天有多少下载量等，这个作为一个大类的特征，然后分为很多小特征。</p>

<p>「选择依据」一个正常软件的sha1值一般是固定的，因为一般软件升级需要一段时间，而恶意软件则需要经常免杀，所以sha1值经常变动，导致每个sha1值的下载量很少，并且恶意软件下载的量一般也会很少。</p>

<h4>域名特征</h4>

<p>主要涉及域名的二级域名，或者域名后缀，同时结合历史数据来计算每一类域名的恶意概率，正常文件对应的数量，每个顶级域名下载量等。</p>

<p>「选择依据」恶意软件虽然经常变换域名，但是很多时候可能只是更换二级域名或者是直接更换定于域名，但是一般情况下后缀不会改变，另外恶意域名对应的下载量应该很小，应该会经常变换域名和文件。</p>

<h4>服务端IP特征</h4>

<p>涉及服务端IP所属BGP，服务器地区，然后跟这几个属性关联的数量，比如某个BGP下下载量是多少，有多少正常文件等等。</p>

<p>「选择依据」恶意文件托管或者存放一般可能会改动IP地址，但是改动不大，可能还是一些BGP下，那么统计其分布情况和数量就非常重要。</p>

<h4>URL特征</h4>

<p>主要考虑URL的构成（路径，特殊符号，参数，文件名等）跟恶意数量和正常样本的数量分布情况，</p>

<p>「选择依据」考虑一个团伙或者一套恶意系统往往存在一些相似的地方，其URL相似度可能比较高。</p>

<h4>下载请求特征</h4>

<p>恶意软件下载的时候请求头，是否带有referer url，路径的深度等于历史。</p>

<p>「选择依据」很多恶意请求基本上referer都是为空，另外恶意文件下载的链接跟正常的一般也不一样，没有jpg,gif这些后缀。</p>

<h3>缺点</h3>

<ul>
<li>虽然作者考虑了很多因素来综合评定文件是否恶意，但是HTTPS没办法测试，除了SSLtrip。</li>
<li>针对分段的恶意木马没办法检测；</li>
<li>恶意攻击者可以针对性改进。</li>
</ul>


<p>后话，作者主页上的<a href="http://roberto.perdisci.com/useful-links">Useful Public Resources</a>对恶意库和检测平台总结还是比较全，推荐。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Understanding and Monitoring Embedded Web Scripts]]></title>
    <link href="http://blog.ourren.com/blog/2015/06/18/understanding-and-monitoring-embedded-web-scripts/"/>
    <updated>2015-06-18T10:26:49-07:00</updated>
    <id>http://blog.ourren.com/blog/2015/06/18/understanding-and-monitoring-embedded-web-scripts</id>
    <content type="html"><![CDATA[<p>该篇文章发表于36th IEEE Symposium on Security and Privacy (&ldquo;Oakland&rdquo;)「属于安全界的顶级会议」，而作者是Yuchen Zhou，毕业于University of Virginia，目前就职Palo Alto Networks。从作者的<a href="http://www.yuchenzhou.info/research">个人主页</a>可以看到其主要的研究内容：通过修改浏览器中特殊功能来实现对单点Web安全技术进行研究。PS：发的顶会文章也是够多的。</p>

<p>该<a href="http://scriptinspector.org/">项目</a>主要开发了一个修改版的Firefox浏览器「ScriptInspector」来帮助网站管理人员查看网站第三方JavaScript对网页内容的相关操作，主要分为三个模块：ScriptInspector，Visualizer和PolicyGenerator。</p>

<ul>
<li>ScriptInspector：基于Firefox的修改版浏览器，主要修改window.eval等函数的hook. 用来记录第三方脚本的函数调用，主要包含：DOM, 本地存储和网络。</li>
<li>Visualizer：一个Firefox插件，用来高亮显示页面中被访问的节点，可以快速了解第三方脚本的操作。</li>
<li>PolicyGenerator：用于辅助人员生成各种第三方脚本的访问策略，通过这些脚本在不同网站的访问情况可以推断该脚本的安全性。</li>
</ul>


<!--more-->


<p>作者通过分析把第三方脚本的操作分为了如下大类：获取浏览器版本，网络请求，修改网页内容，读取网页内容，记录用户操作，脚本插入，获取属性值。在具体实现部分从技术上讲解了这几类操作的实现细节。</p>

<p>在策略分类问题上，主要分为几下几类：统计分析类脚本，广告类脚本，社交类脚本和Web开发类脚本「比如Jquery,google font等」。</p>

<p>在最终测试中发现了很多有意思的东西：Alex 200的网站嵌入第三方脚本很多，并且有些脚本有违规的操作，比如每个网站平均需要6个权限，Facebook和Twitter请求权限很多，并且有时候会读取其它网页内容或者操作行为等，详细可以参考论文的「POLICY EVALUATION」。</p>

<p>相关研究中对客户端脚本安全进行了详细的梳理，可以说非常不错。</p>

<h3>点评</h3>

<p>其实研究第三方脚本的安全隐私的论文已经很多了「可以参考文章中得相关工作和参考文献」，但是一般情况都是直接写一个插件利用JS HOOK来记录脚本的活动「Ghostery，BEEP，MashupOS，OMash」，而基于修改版的Firefox则很少，工业界中<a href="https://dominator.mindedsecurity.com/">DOMinator</a>利用修改版的Firefox来挖掘DOM XSS，而学术界貌似很少。 其实CSP的提出也是用于解决此类安全问题。</p>

<p>另外论文的工作量看起来是比较多的，但是说实话测试数据不是很大，下载使用其程序感觉不够稳定，基本上算demo版本。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Detecting Spammers on Twitter]]></title>
    <link href="http://blog.ourren.com/blog/2015/05/21/detecting-spammers-on-twitter/"/>
    <updated>2015-05-21T19:43:11-07:00</updated>
    <id>http://blog.ourren.com/blog/2015/05/21/detecting-spammers-on-twitter</id>
    <content type="html"><![CDATA[<p>文章主要是通过机器学习的思路来自对twitter的用户进行分类「两类：正常用户与垃圾用户」。其中算法采用的是SVM算法，特征选取比较复杂，结合了内容特征与用户行为特征。个人感觉不错的是论文针对特征的选取和特征的精简部分。</p>

<h3>整体流程</h3>

<ol>
<li>首先爬取了大量Twitter用户的数据（通过API接口 + 大量VPS/IP）；</li>
<li>通过特定主题进行跟踪，例如跟踪：「the Michael Jackson’s death」，「Susan Boyle’s emergence,」，「the hashtag “#musicmon- day”」；即把这几个主题的发言和评论抓出来进行分析；这里实验数据处理的时候有一个技巧：正常内容很多，而垃圾内容太少，为了不让比例失调，特别减少了正常用户的比例，基本上「正常用户:垃圾用户(1：2)」;</li>
<li>手动做标签，作者写分为三个人同时对同一样本做标签，然后标识的时候需要参照三个人的答案；</li>
<li>提取特征值，鉴于特征值比较多，因此需要针对特征值进行分析，分析单一特征是否可以区分两类用户；</li>
<li>通过SVM算法对已经标识的样本进行建模，然后测试样本；</li>
<li>通过分析特征值，降维处理「减少特征值」；</li>
<li>最后计算准确率并分析。</li>
</ol>


<!--more-->


<h3>创新点</h3>

<ol>
<li>从内容特征和用户特征对twitter社交数据进行特征提取，内容特征考虑：最大长度，最小长度，均值，一个内容中的主题(##), 网址数量，单词数量，数字出现个数，@的用户数量，有多少人转发等「39个属性」；用户特征：粉丝数量，关注数量，发帖数量，转帖数量，年龄，被@多少次，多少次回复，发帖时间间隔，一天发多少帖，一周发多少帖「23个属性」。</li>
<li>单一特征对分类的影响采用了cumulative distribution function (CDF)进行了分析，不熟悉CDF的可以看这里<a href="http://www.lifelaf.com/blog/?p=746">一维数据可视化：累积分布函数(Cumulative Distribution Function)</a>;</li>
<li>SVM分类中采用了5阶交叉验证，即SVM参数的不同；</li>
<li>通过信息增益或者X2统计分析TOP10,TOP20,-TOP50特征值「内容特征与用户特征」的数量；关于信息增益可以看这里：<a href="http://www.blogjava.net/zhenandaci/archive/2009/03/24/261701.html">文本分类入门（十一）特征选择方法之信息增益</a>;</li>
<li>然后用这些特征再次测试准确率。</li>
</ol>


<h3>总结</h3>

<p>从工作量上看确实需要做大量的工作，另外作者对特征选取，特征优化这些做得很不错。最后请记住这个是2010年的论文。</p>

<p>论文下载<a href="http://www.decom.ufop.br/fabricio/download/ceas10.pdf">Detecting Spammers on Twitter</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[On the Relations Between the Different Actors in the Spam Landscape]]></title>
    <link href="http://blog.ourren.com/blog/2015/05/11/on-the-relations-between-the-different-actors-in-the-spam-landscape/"/>
    <updated>2015-05-11T17:42:36-07:00</updated>
    <id>http://blog.ourren.com/blog/2015/05/11/on-the-relations-between-the-different-actors-in-the-spam-landscape</id>
    <content type="html"><![CDATA[<p>该篇论文的题目为：<a href="http://dl.acm.org/citation.cfm?id=2590302">The Harvester, the Botmaster, and the Spammer- On the Relations Between the Different Actors in the Spam Landscape</a> 通过分析Harvester，Botmaster和Spammer之间的关系。</p>

<p>其实整个过程可以描述为：攻击者从网络上收集邮箱地址，然后利用Botnet发送推广广告或者恶意内容，从而获利。因此论文中对整个过程进行了重现：</p>

<ul>
<li>自己搭建了网络服务并把邮件地址公布在网络上「采用的是自己搭建的邮件服务器」，等待攻击者来采集邮箱，同时记录下其爬虫的标示「HTTP头，IP地址和其它信息」；</li>
<li>记录邮件服务器软件的收信过程，详细记录投递过程，攻击者在发送邮件的时候会暴露一些特征「使用邮件服务器，握手过程」；</li>
<li>对攻击者发送的内容进行分析「主题，域名，发送软件，发件人」；</li>
<li>通过以上数据对这个攻击链进行分析。</li>
</ul>


<!--more-->


<h3>贡献</h3>

<p>通过阅读论文，发现文章主要有如下的贡献点：</p>

<ul>
<li>通过搭建蜜罐系统来分析这个攻击链，可以说工作量相对比较多，思路也不错；</li>
<li>提出了SMTP通信痕迹概念，其实这个概念作者以前的文章就提出了。按照我的理解就是如果使用自定义搭建的SMTP发信的时候通信过程中会多一些步骤，同时可以通过这个过程来标示一个发信机器；</li>
<li>从邮件内容对恶意攻击进行分类，根据内容分为不同的列别；同时通过了Anubis和virustotal来分析IP的历史数据，是否存在恶意行为；</li>
</ul>


<h3>缺点</h3>

<p>但是个人感觉文章存在以下不足：</p>

<ul>
<li>观测时间太短，从2012.12.14-2013.05.15时间跨度太短，做的网站很可能Google都还没有被收录，攻击者就没有办法获取你的邮箱地址；同时可以发现后面的发件人IP居然只有75个，不得不说数据量确实很少；</li>
<li>还是上面的问题，始终感觉这种方式去逼近地下产业，数据量和切入点都太小了。</li>
</ul>

]]></content>
  </entry>
  
</feed>
